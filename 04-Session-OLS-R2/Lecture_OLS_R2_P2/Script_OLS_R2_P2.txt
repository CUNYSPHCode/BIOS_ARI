This video is part 2 of the lecture on O-L-S and R-squared.

We have learned about Pearson's correlation coefficient in an introductory statistics course. We can calculate Pearson's correlation coefficient between Y and Y-hat, that is, between observed and predicted values of the dependent variable of multiple regression. The correlation coefficient is called the multiple correlation coefficient, and usually denoted by capital R in statistical literature. In this section, we will study two important features of the multiple correlation coefficient. First, it can be shown, that, if the sum of squared residuals, that is, the sum of squared differences between Y and Y-hat, is minimized, then the multiple correlation coefficient is maximized. So, the O-L-S estimation can be considered as a method of finding values of parameters of the regression model, that maximizes correlation between observed and predicted values of the dependent variable. Second, R squared, that is, the squared value of the multiple correlation coefficient, can be considered as the proportion of variance of Y, that is predicted by the regression model. Instead of variance, we can say, the proportion of the sum of squared deviations, which is the numerator of variance. Also, instead of "predicted by the regression model", some may say, "estimated by the regression model", or "explained by the regression model".

Let's look at this example. A regression analysis of systolic blood pressure of adult women, on age and education, was done, and these parameter values were obtained. In this equation, Y is the systolic blood pressure, X1 is age, x2 is a binary variable for those who have not graduated from high school, and X3 is a binary variable for those who have graduated from high school, but not from college.

Let's think of a woman who is 73 years old, and has graduated from high school, but not from college. So, her value of X1 is 73, X2 is zero, and X3 is one. Substituting these values into the estimated regression equation, we get 140 as her Y-hat, that is, the predicted value of her systolic blood pressure, and the prediction was based on her age and education. But her Y, the observed value of her systolic blood pressure is 150. So, her residual is, 150 minus 140 which equals 10.

This graph is a plot of Y against Y-hat from the regression analysis. The redline is Y equals Y-hat. For data points that are on the redline, Y and Y-hat values are identical, that is, the prediction was completely accurate for those individuals. The green circle indicates the woman we talked about in the previous slide, whose predicted value of systolic blood pressure was 140, and observed value was 150. The pattern of scattering seems to indicate that, the correlation coefficient between Y and Y-hat, should be positive, and neither very low nor very high.

Actually, the correlation coefficient, R, is .594, and its squared value, R square, is .353,  as shown in this table. Computer outputs of regression analyses usually show R, and always show R square.

This sub-section focuses on the proportion of variance, predicted by the regression model. It is also the proportion of the sum of squared deviations from the mean, which is the numerator of variance, predicted by the regression model. The word "explained" is more widely used than "predicted", but "predicted" seems more appropriate in this context, because this is simply a matter of, how closely two sets of values are close to each other.

Let's think of this issue, using the example of the 73-year old woman.  Her systolic blood pressure is 150, its predicted value based on her age and education, is 140, and the mean systolic blood pressure of the entire sample, denoted by Y-bar, is 123.  What would our best guess of our systolic blood pressure be, if we have no information on her, except for the fact that she is a case in the sample? In that case, it is just 123, isn't it? With information on her age and education, our guess of her blood pressure changed from 123 to 140, becoming closer to the true value of 150 by 17. But it is still 10 points away, from the true value of 150. So, if she is represented as individual i,  Y-i minus Y-hat-i, is 10 , and Y-hat-i minus Y-bar, is 17, and their sum, 27, is the total deviation of her observed value from the mean. In other words, as shown by the equation at the lower left corner to the slide, Y-i minus Y-bar, can be expressed as the sum of two terms, Y-i minus Y-hat-i, and Y-hat-i minus Y-bar. Note that the first term, Y-i minus Y-hat-i, the residual, is a prediction error because it is the difference between the observed and predicted values.

Now, since Y-i minus Y-hat-i squared, is used in OLS regression, let's think about  squared values of the two terms, that is, Y-i minus Y-hat-i squared, and Y-hat-i minus Y-bar squared. As the second equation shows, Y-i minus Y-bar, the deviation from the mean, can be expressed as the sum of Y-i minus Y-hat-i, and Y-hat-i minus Y-bar. But, as shown by the third equation,  Y-i minus Y-bar squared, cannot be expressed as the sum of Y-i minus Y-hat-i squared, and Y-hat-i minus Y-bar squared. As we studied in high school algebra, we get a third term, that is, two times the product of the two terms, Y-i minus Y-hat-i and Y-hat-i minus Y-bar.

But the relation becomes simpler, if we get the sum, over all cases, of each of these terms, because it can be shown, that the sum of Y-i minus Y-hat-i and Y-hat-i minus Y-bar becomes zero, and thus disappear. Why does the sum become zero? There is a mathematical proof, but it comes from what we studied in part 1. At the end of part 1, we studied about some important characteristics of O-L-S estimates, and the third characteristic was the statistical independence, that is, zero correlation, between the residual and Y-hat-i.    So, now we have the equation at the bottom, (the sum of squared differences between Y-i and Y-bar equals (the sum of squared differences between Y-i and Y-hat-i and (the sum of squared differences between Y-hat-i and Y-bar. In some textbooks, this relation may be expressed as T S S equals R S S plus regression S S.

By dividing all terms of the equation at the bottom of the previous slide, by “n”, that is, the number of cases, we get this equation at the top. What is the term on the left-hand side?  It is the sum of squared differences between Y and the mean Y, divided by n, so this is the variance of Y. The second term on the right-hand side is the variance of Y hat, because as we studied in part one, the mean of Y hat is the same as the mean of Y. The first term on the right-hand side is the variance of residual. You may feel that it doesn't look like a variance. Where is the mean value? Y-hat-i is not the mean of anything. Well, we don't need to show a mean value here, because the mean value is zero. Y-i minus Y hat-i is the residual, and as we learned in part one, the mean of residuals is zero. So we got this second equation in this slide, nice and simple, the variance of Y equals the variance of E plus the variance of Y-hat. Since E is the residual, which can be considered the prediction error, the variance of E can be considered a part of variance of Y that represents prediction errors, so the remainder, which is the variance of Y-hat, may be considered to a part of variance of Y representing the extent of prediction success. Since the variance of Y, can be split into the variance of E and the variance of Y-hat, the ratio of variance of E to variance of Y, can be interpreted as, the proportion of variance of Y, that was not captured by regression, and the ratio of variance of Y-hat to variance of Y, can be interpreted as, the proportion of variance of Y that was captured by regression. By the way, in this slide, the variance was obtained by dividing the sum of squared deviations by N. As we studied in introductory statistics, there are two formulas for variance, one uses N, and the other uses N minus 1. They are for different purposes, but the difference does not matter here, as we get the same results, whether we use N or N minus 1.

This split of variance is related to the multiple correlation coefficient. As we learned in part one, the multiple correlation coefficient, is Pearson's correlation coefficient between Y and Y-hat. This plot shows the relation between Y and Y hat from some regression analysis. The correlation coefficient is 0.594, and its squared value, denoted as R squared, is 0.594 times 0.594, which is 0.353.

Mathematical statisticians have shown that this R squared, is equal to the ratio of variance of Y-hat to variance of Y. Since the variance of Y can be split into the variance of E and  the variance of Y-hat, one minus R squared is equal to the ratio of variance of E to variance of Y. So, the squared multiple correlation coefficient, called R square or R squared, can be interpreted as the proportion of variance of Y predicted by regression. In O-L-S, the sum of squared residuals, which is the numerator of the variance of E, is minimized. Since the variance of Y can be split into the variance of E and the variance of Y hat, the minimization of the variance of E, means maximization of the variance of Y hat, that is, maximization of R squared.

So, R squared is a measure of predictive success of linear regression analysis. Its value ranges from 0 to 1, because it is a squared correlation coefficient, and because it is a proportion. How large should the value of R squared be? To think about this question, we need to understand that there are two major objectives of regression analysis. The first objective is prediction. If we are concerned about two variables, X and Y, and if we often come across a situation in which we have information on X, but not on Y, then it is useful to have an equation for predicting Y from X. So, when we have data on both X and Y, we set up an equation predicting Y from X, and then, when we have data on X only, we use the equation to predict the value of Y. For example, a high level of P-S-A, prostate specific antigen, in blood, suggests that the person may have prostate cancer. So, when we have data on blood test results and the prostate cancer status for several males, we set up an equation for predicting the prostate cancer status from the level of P-S-A. Then, when we have blood test results of a patient, but we don't know if he has a prostate cancer or not, the equation should be useful, for determining whether the patient should have a biopsy for examining his prostate cancer status. By the way, actually this is not an example of linear regression, because Y is a dichotomous variable, that is, whether the patient has prostate cancer or not, which is not a continuous numerical variable that the dependent variable of linear regression should be. This example is more suitable for logistic regression analysis, but the logic is essentially the same.

The other major objective of regression analysis, is estimation of the relation between X and Y. The regression coefficient of X, indicates, on average, how much difference in Y is associated with one-unit difference in X. If you assume that there is a causal relation between X and Y, and X causes Y, then the regression coefficient of X may be considered to show, on average, how much difference in Y is produced by one unit difference in X.

So, if the primary purpose of the regression analysis is prediction, we should be strongly concerned about the prediction accuracy, which is measured by R squared. But if the primary purpose is estimation of the relation between X and Y, we are usually more concerned about the size, sign, substantive significance, and statistical significance of the regression coefficient, B, of X, than the value of R-squared. If there are several other strong determinants of Y, and if quite a few of them are not included in the model, then the value of R squared may be relatively low. But you shouldn't be too discouraged by the value, as the focus is on the regression coefficient of a particular independent variable, or variables.
