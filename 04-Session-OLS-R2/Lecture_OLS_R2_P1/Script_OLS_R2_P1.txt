In this session, we study about O-L-S, ordinary least squares, and R squared, which is also called R square.

This is the model of multiple linear regression that we studied in the last session. We focused on how to interpret those beta coefficients. Then the question is, how are values of these coefficients estimated? Also, since this is a statistical model, it is fitted to data. Then how do we measure the goodness of fit, that is, the extent to which the model fits the data well? In order to study these questions, we will learn about O-L-S, which is the most widely used method for estimating values of parameters of linear regression models, and R squared, which is the most widely used measure of goodness of fit of linear regression models to data. R squared represents the proportion of variance predicted or explained by the regression analysis. We also will study why R squared is considered so.

In this session, first we will study about O-L-S, ordinary least squares, and then about R square, that is, squared multiple correlation coefficient. Lastly, two major objectives of regression analysis are discussed in relation to the major topics of this session.

O-L-S or ordinary least squares is the standard method for estimating values of parameters, usually denoted as alpha and betas, of a linear regression model. The first equation is the model of multiple linear regression. The subscript i, indicates the i-th individual. So (y sub i) is the value of the dependent variable for individual i, and (x sub i 1) is the value of the first dependent variable for individual “I”. Note that the values of Y, the Xs, and epsilon differ among individuals, whereas the values of alpha and betas are the same for all individuals. The second equation represents an estimated regression model. The first and second equations look essentially the same, but the difference between alpha, betas, and epsilon, on the one hand, and “A”, Bs, and E, on the other, are that alpha, betas, and epsilon represent the true values, and “A”, Bs, and E, represent their estimated values. How are these parameter values estimated? In a nutshell, they are estimated to make the model fit the data well.

Let's start with a case of one independent variable. The left panel is a plot of systolic blood pressure by age. The points are scattered, but there seems to be our wide band of points moving up towards from left to right. The redline is a straight line fitted to the data. The right panel shows the same data, but a steeper redline was fitted. The fit doesn't look too bad. But which line fits the data better?

Let's think about this question with simple artificial data. In this graph, the higher straight line was fitted to the three points. Obviously, the upper line fits the points better than this lower line. Why? --Because the upper line is closer to the point than the lower line is. So an important criterion for assessing the goodness of fit is distances between points and the line.

The distance is expressed as (Y sub I) minus (Y-hat sub I). In this graph, the black circle indicates an individual whose age is 64, and whose blood pressure is 152. The diagonal straight-line is the regression line, which is expressed as (Y hat equals 100.78 plus 0.516 times X). The regression model predicts the systolic blood pressure of a person aged 64, as 100.78 plus 0.516 times 64, which is about 134. The combination of 64 and 134 is represented by the white circle on the regression line. So, for this person (i), the observed systolic blood pressure, Y-i, is 152, and the predicted systolic blood pressure, Y-hat-i, is 134. The length of the red vertical line is, (Y-i) minus (Y-hat-i), which is the difference between the observed value and predicted value of systolic blood pressure for individual “I”. In regression analysis, this difference is called a residual. A smaller residual means that the point is closer to the line, so the regression model fits the data better. But this slide shows just one data point, represented by the black circle. A data set usually has data on multiple individuals. So we need to find a straight line that minimizes a value of some criterion summarizing the fit of the regression line to all those points.

Three quantities are listed in the slide as candidates for such summary measure. The first quantity that we can think of, is the sum of all residuals. But this quantity has a problem that positive values and negative values may cancel each other to some extent. For example, suppose there are three data points and their residuals are four, negative six, and two. This means, the first and third data points are above the regression line, and the second data point is below the regression line. But four minus six plus two equals zero, which is the same as a perfect fit. The second quantity listed here is the sum of absolute values of residuals, which can circumvent the problem of the first quantity. This criterion seems to make sense, but computation with many absolute values could be quite messy even with our P-Cs of today. Also, it can be shown that it is possible for two or more sets of parameter estimates produce the same minimum value of this quantity. In that case, we cannot determine which straight line to choose. The third quantity listed here is the sum of squared residuals. This also can circumvent the problem of the first quantity, and computationally squared residuals are easier to handle than absolute residuals. Furthermore, values of the parameters that minimize this quantity can be uniquely determined. So this quantity, the sum of squared residuals, is a widely used for regression analysis, that is, values of the parameters of the regression model that minimize this quantity, are adopted as estimated parameter values.

Mathematically, the idea can be represented as shown here. The equation at the top is an estimated model of simple linear regression. Y sub i, the observed value of the dependent variable for individual I, is Y-hat sub i, it's predicted value, plus E-i, the difference between the the observed and predicted values. The value of Y predicted by simple linear regression, is expressed as “A” plus (B) times (x-i), so Y sub i, is expressed as “A” plus B times (x-i) plus E-i. We select values of A and B that minimize the sum of squared residuals, which is denoted as (R-S-S), and also the sum over all individuals, of E-squared, which can be expressed as the sum over all individuals, of this squared quantity, (Y-i, minus A, minus B times X-I). So the sum of squared residuals is determined by Y, X, A, and B. We cannot change values of Y or X, because they are data. But we can change values of “A” and B, to make (R-S-S), the sum of squared residuals, as small as possible.

How can we find values of A and B that minimize R-S-S? We can do so using elementary calculus. Calculus is not a prerequisite for this course. Please note that there is an asterisk at the beginning of the title of this slide. In this course, the asterisk means that the slide is for those who are interested in the particular issue, but not everyone has to understand it. In calculus, we learned that if we have the equation for a smooth curve, the slope of the curve, called the derivative, at any point of the curve can be calculated, and usually denoted by d-y over d-x, where Y is the vertical axis and X is the horizontal axis. We also learned that the derivative is zero at a peak or trough of the curve. You may be puzzled by this graph, because we are studying straight line relationships between Y and X, not such curvilinear relationships. Actually, Y and X in this graph are not the dependent and independent variables of regression. Y is R-S-S, the sum of squared residuals, and X is the parameter A or B. Mathematicians have shown that the relation between R-S-S and the parameter is like the pattern in the right panel, that is, R-S-S has the minimum value, and we can get the minimum value by adjusting values of A and B.

Mathematically, values of A and B that minimize R-S-S can be obtained as shown here. R-S-S is expressed as a function of A and B.  In order to get the minimum value of R-S-S, we set both Delta R-S-S over Delta A and Delta R-S-S over Delta B to be zero. Again, both the derivative of R-S-S with respect to A and the derivative of R-S-S with respect to B are set to be zero. This leads to two simultaneous equations, and we can get values of A and B, by solving the simultaneous equations. By the way, a special variant of letter D, is used here for the derivatives. The regular D, is used for a function of only one variable, like X, but this D, is used for a function of two or more variables, like A and B here, and called the partial derivative symbol.

The values of A and B obtained in this way, determines the straight line which is shown in red in the left panel. This is the line, that minimizes the sum of squared vertical distances, between the points and the line.

So far, we have been studying about the O-L-S method for linear regression, with one independent variable, X. The method can be extended to linear regression, with two or more independent variables, that is, multiple linear regression. At the top of this slide, is the equation for estimated multiple linear regression model. It just has a number of Bs and Xs. R-S-S can be expressed as shown here, with more Bs and Xs.

Now, R-S-S can be considered as a function of A and Bs, because values of Y and Xs are fixed as data. By setting the partial derivatives of R-S-S with respect to A and Bs, we get a set of simultaneous equations, and by solving those simultaneous equations, we obtain values of A and Bs that minimize R-S-S.

This is a plot of Y by Y hat, the observed value of the dependent variable, by its corresponding predicted value. As shown in the equation, the value of Y-hat, is determined by values of A and Bs. In O-L-S estimation, values of A and Bs, that minimizes the sum over all the points of the squared differences between Y and Y hat, are selected as estimated values of alpha and betas. In this graph, as an example, the distance between Y and Y hat for a point, is shown as the length of a vertical line segment, between a gray circle and a green circle.

Some characteristics of O-L-S estimates, are important. First, the mean of Y and the mean of Y hat, are identical. It makes sense, that the mean of observed values and the mean of their predicted values, are the same. Second, the mean of residuals, is zero. Since the mean of Y and the mean of Y hat are equal, the mean of Y minus Y-hat should be zero. Third, Y hat and the residual are independent, that is, the correlation coefficient between them, is zero. This can be shown mathematically, but conceptually, this may be understood as follows. Y-hat is a predicted value of Y, and the prediction is based on the independent variables, Xs. So variations in Y hat represent variations in Y that are related to Xs, whereas variations in the residual represent variations in Y that are unrelated to the Xs. So it makes sense that, Y hat and the residual, are unrelated.
